{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d49fceb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mxgboost\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mxgb\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "data = pd.read_csv(\"Dry_Bean_Dataset.csv\")\n",
    "\n",
    "# Assuming the last column is the label\n",
    "X = data.iloc[:, :-1]\n",
    "y = data.iloc[:, -1]\n",
    "\n",
    "# Convert labels to integer values\n",
    "classes = np.unique(y)\n",
    "y_int = np.array([np.where(classes == label)[0][0] for label in y])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_int, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create DMatrix, the internal data structure that XGBoost uses\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# Define parameters for classification\n",
    "params = {\n",
    "    'objective': 'multi:softmax',  # for multiclass classification\n",
    "    'num_class': len(classes),\n",
    "    'eta': 0.1,\n",
    "    'max_depth': 4\n",
    "}\n",
    "num_round = 20  # number of boosting rounds\n",
    "\n",
    "# Train the model\n",
    "bst = xgb.train(params, dtrain, num_round)\n",
    "\n",
    "# Make predictions\n",
    "predictions = bst.predict(dtest)\n",
    "# Make predictions on the training data\n",
    "train_predictions = bst.predict(dtrain)\n",
    "print(\"Train accuracy:\", accuracy_score(y_train, train_predictions))\n",
    "print(\"Predictions:\", predictions)\n",
    "print(\"test accuracy:\", accuracy_score(y_test, predictions))\n",
    "print(\"test data report:\", classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a680b0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with eta=0.1, max_depth=4...\n",
      "Validation accuracy: 0.9334 (best iteration: 97)\n",
      "Training with eta=0.1, max_depth=5...\n",
      "Validation accuracy: 0.9302 (best iteration: 111)\n",
      "Training with eta=0.1, max_depth=6...\n",
      "Validation accuracy: 0.9323 (best iteration: 85)\n",
      "Training with eta=0.3, max_depth=4...\n",
      "Validation accuracy: 0.9292 (best iteration: 39)\n",
      "Training with eta=0.3, max_depth=5...\n",
      "Validation accuracy: 0.9339 (best iteration: 27)\n",
      "Training with eta=0.3, max_depth=6...\n",
      "Validation accuracy: 0.9276 (best iteration: 25)\n",
      "Training with eta=0.5, max_depth=4...\n",
      "Validation accuracy: 0.9328 (best iteration: 21)\n",
      "Training with eta=0.5, max_depth=5...\n",
      "Validation accuracy: 0.9276 (best iteration: 16)\n",
      "Training with eta=0.5, max_depth=6...\n",
      "Validation accuracy: 0.9292 (best iteration: 13)\n",
      "\n",
      "Best hyperparameters found:\n",
      "{'objective': 'multi:softmax', 'num_class': 7, 'eta': 0.3, 'max_depth': 5, 'best_iteration': 27}\n",
      "Best validation accuracy: 0.9338929695697796\n",
      "\n",
      "Test Accuracy: 0.9258080313418218\n",
      "Test Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.92      0.92       395\n",
      "           1       1.00      1.00      1.00       161\n",
      "           2       0.94      0.94      0.94       479\n",
      "           3       0.91      0.92      0.91      1043\n",
      "           4       0.98      0.96      0.97       588\n",
      "           5       0.95      0.94      0.94       619\n",
      "           6       0.87      0.88      0.88       799\n",
      "\n",
      "    accuracy                           0.93      4084\n",
      "   macro avg       0.94      0.94      0.94      4084\n",
      "weighted avg       0.93      0.93      0.93      4084\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv(\"Dry_Bean_Dataset.csv\")\n",
    "# Assuming the last column is the label\n",
    "X = data.iloc[:, :-1]\n",
    "y = data.iloc[:, -1]\n",
    "# Convert labels to integer values\n",
    "classes = np.unique(y)\n",
    "y_int = np.array([np.where(classes == label)[0][0] for label in y])\n",
    "\n",
    "# Split data into train+val and test (70/30 split)\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(X, y_int, test_size=0.3, random_state=42)\n",
    "# Further split train+val into training and validation (80/20 split)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create DMatrix objects\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dval = xgb.DMatrix(X_val, label=y_val)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# Hyperparameter grid\n",
    "etas = [0.1, 0.3, 0.5]\n",
    "max_depths = [4, 5, 6]\n",
    "num_boost_round = 200      # maximum rounds to allow early stopping to decide the best round\n",
    "early_stopping_rounds = 10\n",
    "\n",
    "best_val_accuracy = 0.0\n",
    "best_params = None\n",
    "best_model = None\n",
    "best_iteration = None\n",
    "\n",
    "# Grid search over eta and max_depth using early stopping on the validation set\n",
    "evals = [(dtrain, 'train'), (dval, 'eval')]\n",
    "for eta in etas:\n",
    "    for max_depth in max_depths:\n",
    "        params = {\n",
    "            'objective': 'multi:softmax',\n",
    "            'num_class': len(classes),\n",
    "            'eta': eta,\n",
    "            'max_depth': max_depth\n",
    "        }\n",
    "        print(f\"Training with eta={eta}, max_depth={max_depth}...\")\n",
    "        bst = xgb.train(\n",
    "            params,\n",
    "            dtrain,\n",
    "            num_boost_round=num_boost_round,\n",
    "            evals=evals,\n",
    "            early_stopping_rounds=early_stopping_rounds,\n",
    "            verbose_eval=False\n",
    "        )\n",
    "        # Evaluate on the validation set\n",
    "        val_pred = bst.predict(dval, iteration_range=(0, bst.best_iteration))\n",
    "        val_acc = accuracy_score(y_val, val_pred)\n",
    "        print(f\"Validation accuracy: {val_acc:.4f} (best iteration: {bst.best_iteration})\")\n",
    "        \n",
    "        if val_acc > best_val_accuracy:\n",
    "            best_val_accuracy = val_acc\n",
    "            best_params = params.copy()\n",
    "            best_params['best_iteration'] = bst.best_iteration\n",
    "            best_model = bst\n",
    "            best_iteration = bst.best_iteration\n",
    "\n",
    "print(\"\\nBest hyperparameters found:\")\n",
    "print(best_params)\n",
    "print(\"Best validation accuracy:\", best_val_accuracy)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "test_pred = best_model.predict(dtest, iteration_range=(0, best_iteration))\n",
    "print(\"\\nTest Accuracy:\", accuracy_score(y_test, test_pred))\n",
    "print(\"Test Classification Report:\\n\", classification_report(y_test, test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aec6324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\angel\\.conda\\envs\\pytorch\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [12:00:36] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters found:\n",
      "{'eta': 0.1, 'max_depth': 5, 'n_estimators': 100}\n",
      "Best cross-validation score: 0.9255996409539717\n",
      "\n",
      "Test Accuracy: 0.9258080313418218\n",
      "Test Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.92      0.93       395\n",
      "           1       1.00      1.00      1.00       161\n",
      "           2       0.95      0.95      0.95       479\n",
      "           3       0.91      0.91      0.91      1043\n",
      "           4       0.97      0.96      0.96       588\n",
      "           5       0.95      0.94      0.95       619\n",
      "           6       0.86      0.88      0.87       799\n",
      "\n",
      "    accuracy                           0.93      4084\n",
      "   macro avg       0.94      0.94      0.94      4084\n",
      "weighted avg       0.93      0.93      0.93      4084\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv(\"Dry_Bean_Dataset.csv\")\n",
    "# Assuming the last column is the label\n",
    "X = data.iloc[:, :-1]\n",
    "y = data.iloc[:, -1]\n",
    "\n",
    "# Convert labels to integer values\n",
    "classes = np.unique(y)\n",
    "y_int = np.array([np.where(classes == label)[0][0] for label in y])\n",
    "\n",
    "# Split data into train+val and test (70/30 split)\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(X, y_int, test_size=0.3, random_state=42)\n",
    "# Further split train+val into training and validation (80/20 split)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an XGBClassifier that integrates with scikit-learn.\n",
    "# Note: use_label_encoder=False and an eval_metric are recommended for newer XGBoost versions.\n",
    "clf = XGBClassifier(objective='multi:softmax',\n",
    "                    num_class=len(classes),\n",
    "                    use_label_encoder=False,\n",
    "                    eval_metric='mlogloss')\n",
    "\n",
    "# Define hyperparameter grid. Here we include 'eta' (learning_rate), 'max_depth', and 'n_estimators'\n",
    "param_grid = {\n",
    "    'eta': [0.1, 0.3, 0.5],\n",
    "    'max_depth': [4, 5, 6],\n",
    "    'n_estimators': [100, 200]  # equivalent to number of boosting rounds\n",
    "}\n",
    "\n",
    "# Setup GridSearchCV.\n",
    "# Extra fit arguments are passed via 'fit_params'\n",
    "grid = GridSearchCV(estimator=clf,\n",
    "                    param_grid=param_grid,\n",
    "                    scoring='accuracy',\n",
    "                    cv=3,\n",
    "                    n_jobs=-1,\n",
    "                    verbose=1)\n",
    "\n",
    "# Fit GridSearchCV. Use the validation set for early stopping.\n",
    "# fit_params = {\n",
    "#     \"eval_set\": [(X_val, y_val)],\n",
    "#     \"early_stopping_rounds\": 10,\n",
    "#     \"verbose\": False\n",
    "# }\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best hyperparameters found:\")\n",
    "print(grid.best_params_)\n",
    "print(\"Best cross-validation score:\", grid.best_score_)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_model = grid.best_estimator_\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "print(\"\\nTest Accuracy:\", accuracy_score(y_test, y_test_pred))\n",
    "print(\"Test Classification Report:\\n\", classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac22001e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.66896\teval-mlogloss:1.67187\n",
      "[1]\ttrain-mlogloss:1.46646\teval-mlogloss:1.47224\n",
      "[2]\ttrain-mlogloss:1.30796\teval-mlogloss:1.31615\n",
      "[3]\ttrain-mlogloss:1.17852\teval-mlogloss:1.18869\n",
      "[4]\ttrain-mlogloss:1.06917\teval-mlogloss:1.08118\n",
      "[5]\ttrain-mlogloss:0.97606\teval-mlogloss:0.98974\n",
      "[6]\ttrain-mlogloss:0.89505\teval-mlogloss:0.91018\n",
      "[7]\ttrain-mlogloss:0.82346\teval-mlogloss:0.84023\n",
      "[8]\ttrain-mlogloss:0.76029\teval-mlogloss:0.77846\n",
      "[9]\ttrain-mlogloss:0.70459\teval-mlogloss:0.72420\n",
      "[10]\ttrain-mlogloss:0.65455\teval-mlogloss:0.67531\n",
      "[11]\ttrain-mlogloss:0.61048\teval-mlogloss:0.63209\n",
      "[12]\ttrain-mlogloss:0.57058\teval-mlogloss:0.59305\n",
      "[13]\ttrain-mlogloss:0.53454\teval-mlogloss:0.55827\n",
      "[14]\ttrain-mlogloss:0.50213\teval-mlogloss:0.52679\n",
      "[15]\ttrain-mlogloss:0.47270\teval-mlogloss:0.49821\n",
      "[16]\ttrain-mlogloss:0.44611\teval-mlogloss:0.47270\n",
      "[17]\ttrain-mlogloss:0.42170\teval-mlogloss:0.44917\n",
      "[18]\ttrain-mlogloss:0.39975\teval-mlogloss:0.42818\n",
      "[19]\ttrain-mlogloss:0.37975\teval-mlogloss:0.40920\n",
      "[20]\ttrain-mlogloss:0.36118\teval-mlogloss:0.39140\n",
      "[21]\ttrain-mlogloss:0.34447\teval-mlogloss:0.37525\n",
      "[22]\ttrain-mlogloss:0.32918\teval-mlogloss:0.36054\n",
      "[23]\ttrain-mlogloss:0.31531\teval-mlogloss:0.34747\n",
      "[24]\ttrain-mlogloss:0.30251\teval-mlogloss:0.33512\n",
      "[25]\ttrain-mlogloss:0.29081\teval-mlogloss:0.32411\n",
      "[26]\ttrain-mlogloss:0.27995\teval-mlogloss:0.31382\n",
      "[27]\ttrain-mlogloss:0.27009\teval-mlogloss:0.30485\n",
      "[28]\ttrain-mlogloss:0.26091\teval-mlogloss:0.29619\n",
      "[29]\ttrain-mlogloss:0.25254\teval-mlogloss:0.28847\n",
      "[30]\ttrain-mlogloss:0.24466\teval-mlogloss:0.28133\n",
      "[31]\ttrain-mlogloss:0.23753\teval-mlogloss:0.27467\n",
      "[32]\ttrain-mlogloss:0.23093\teval-mlogloss:0.26862\n",
      "[33]\ttrain-mlogloss:0.22487\teval-mlogloss:0.26294\n",
      "[34]\ttrain-mlogloss:0.21923\teval-mlogloss:0.25779\n",
      "[35]\ttrain-mlogloss:0.21407\teval-mlogloss:0.25317\n",
      "[36]\ttrain-mlogloss:0.20927\teval-mlogloss:0.24888\n",
      "[37]\ttrain-mlogloss:0.20480\teval-mlogloss:0.24476\n",
      "[38]\ttrain-mlogloss:0.20063\teval-mlogloss:0.24114\n",
      "[39]\ttrain-mlogloss:0.19670\teval-mlogloss:0.23762\n",
      "[40]\ttrain-mlogloss:0.19317\teval-mlogloss:0.23452\n",
      "[41]\ttrain-mlogloss:0.18981\teval-mlogloss:0.23157\n",
      "[42]\ttrain-mlogloss:0.18658\teval-mlogloss:0.22902\n",
      "[43]\ttrain-mlogloss:0.18377\teval-mlogloss:0.22647\n",
      "[44]\ttrain-mlogloss:0.18096\teval-mlogloss:0.22426\n",
      "[45]\ttrain-mlogloss:0.17822\teval-mlogloss:0.22204\n",
      "[46]\ttrain-mlogloss:0.17585\teval-mlogloss:0.22002\n",
      "[47]\ttrain-mlogloss:0.17354\teval-mlogloss:0.21818\n",
      "[48]\ttrain-mlogloss:0.17139\teval-mlogloss:0.21643\n",
      "[49]\ttrain-mlogloss:0.16939\teval-mlogloss:0.21491\n",
      "Train accuracy: 0.9425842342815157\n",
      "Test accuracy: 0.9260528893241919\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.91      0.92       395\n",
      "           1       1.00      1.00      1.00       161\n",
      "           2       0.94      0.94      0.94       479\n",
      "           3       0.91      0.93      0.92      1043\n",
      "           4       0.97      0.95      0.96       588\n",
      "           5       0.95      0.94      0.94       619\n",
      "           6       0.88      0.88      0.88       799\n",
      "\n",
      "    accuracy                           0.93      4084\n",
      "   macro avg       0.94      0.94      0.94      4084\n",
      "weighted avg       0.93      0.93      0.93      4084\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split,cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "data = pd.read_csv(\"Dry_Bean_Dataset.csv\")\n",
    "\n",
    "# Assuming the last column is the label\n",
    "X = data.iloc[:, :-1]\n",
    "y = data.iloc[:, -1]\n",
    "\n",
    "# Convert labels to integer values\n",
    "classes = np.unique(y)\n",
    "y_int = np.array([np.where(classes == label)[0][0] for label in y])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_int, test_size=0.3, random_state=42)\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "params = {\n",
    "    'objective': 'multi:softmax',  # multiclass classification\n",
    "    'num_class': len(classes),\n",
    "    'eta': 0.1,           # try reducing the learning rate\n",
    "    'max_depth': 4,       # adjust by lowering to reduce overfitting\n",
    "    # You can experiment further by adding 'gamma', 'subsample', etc.\n",
    "}\n",
    "num_round = 50  # Increase boosting rounds to see the effect with early stopping\n",
    "\n",
    "evals = [(dtrain, \"train\"), (dtest, \"eval\")]\n",
    "bst = xgb.train(params, dtrain, num_boost_round=num_round, evals=evals, early_stopping_rounds=5)\n",
    "\n",
    "train_predictions = bst.predict(dtrain)\n",
    "print(\"Train accuracy:\", accuracy_score(y_train, train_predictions))\n",
    "\n",
    "predictions = bst.predict(dtest)\n",
    "print(\"Test accuracy:\", accuracy_score(y_test, predictions))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19159008",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split,cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "data = pd.read_csv(\"Dry_Bean_Dataset.csv\")\n",
    "\n",
    "# Assuming the last column is the label\n",
    "X = data.iloc[:, :-1]\n",
    "y = data.iloc[:, -1]\n",
    "\n",
    "# Convert labels to integer values\n",
    "classes = np.unique(y)\n",
    "y_int = np.array([np.where(classes == label)[0][0] for label in y])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_int, test_size=0.3, random_state=42)\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "params = {\n",
    "    'objective': 'multi:softmax',  # multiclass classification\n",
    "    'num_class': len(classes),\n",
    "    'eta': 0.1,           # try reducing the learning rate\n",
    "    'max_depth': 4,       # adjust by lowering to reduce overfitting\n",
    "    # You can experiment further by adding 'gamma', 'subsample', etc.\n",
    "}\n",
    "num_round = 50  # Increase boosting rounds to see the effect with early stopping\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
